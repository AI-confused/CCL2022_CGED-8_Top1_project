data_dir: /home/liyunliang/CGED_Task/dataset/
exp_dir: /home/liyunliang/CGED_Task/output/
# model_name: /home/liyunliang/pretrained_model/chinese_wwm_ext_pytorch/
# model_name: /home/liyunliang/pretrained_model/bart_chinese_base/
model_name: /home/liyunliang/pretrained_model/bart_chinese_large/
task_name: ' Grammar_correct_task_ptr-gen_bart-large_5e-6_lang8+liinan-base_linian_cosine_drop0.1_beam3_check'
# task_name: 'Grammar_correct_task_s2a_bart-large_5e-5_lang8-all_drop0.1_cosine_alpha0.01'
# task_name: 'Grammar_correct_task_s2a_bart-base_5e-6_lang8-all-base_14-18_simply_train+test_char_drop0.3_cosine_alpha0.01'
# task_name: 'Grammar_correct_task_s2a_bart-large_5e-5_lang8-all-base_14-18_simply_train+test_char_drop0.3_cosine_alpha0.01'
skip_train: 0
# resume_model_path: /home/liyunliang/NBME/output/NBME_task_roberta_large_f0/Model/NBME_task_roberta_large.cpt.0
# case_feature_text_dict_file: /home/liyunliang/NBME/dataset/case_feature_text_dict.json
# return_offsets_mapping: True
scheduler: cosine
# alpha: 0.001
beam_k: 3
# adverse_train: fgm
warmup_portion: 0.0
eval_portion: 0.5
# truncation: only_second
# padding: max_length
train_file_name: jianti_14-18_train+test_charlevel/detect_all_data_v3.jsonl
# train_file_name: lang8+linian/train.jsonl
dev_file_name: jianti_14-18_train+test_charlevel/detect_dev_v2.jsonl
# test_file_name: dev.csv
load_test: 0
# evaluation_metric: 'posi_f1'
# threshold: 0.5
dropout: 0.1
max_seq_len: 256
train_batch_size: 256
eval_batch_size: 32
learning_rate: 5e-6
num_train_epochs: 10
no_cuda: False
# specify the GPU number
cuda_device: '2,3,4,5'
seed: 99
gradient_accumulation_steps: 8
over_write_cache: 1
resume_latest_cpt: 0
bad_case: 1
# save_cpt_flag value: {0: only save best model; 1: save best model & last epoch model; 2: save best model & each epoch model}
save_cpt_flag: 2
percent: 1.0