data_dir: /home/liyunliang/CGED_Task/dataset/
exp_dir: /home/liyunliang/CGED_Task/output/
model_name: /home/liyunliang/pretrained_model/chinese_macbert_large/
# task_name: 'Grammar_detect_task_macbert-large_1e-5_lang8-all_drop0.1_cosine_bio13_rdrop(10)'
# task_name: 'Grammar_detect_task_macbert-large_bilstm+crf_5e-6_lang8-all-base_14-18_simply_train+test_char_drop0.5_cosine(0.1)'
# task_name: 'Grammar_detect_task_macbert-large_1e-5_lang8-aug_drop0.1_cosine'
task_name: 'Grammar_detect_task_macbert-large_bilstm+crf_1e-5_lang8-all_drop0.1_cosine_check'
skip_train: 0
scheduler: cosine
# adverse_train: fgm
# freeze_layers:
#   - bert
warmup_portion: 0.0
eval_portion: 0.5
# train_file_name: jianti_14-18_train+test_charlevel/detect_all_data_v3.jsonl
train_file_name: lang8/lang8_data_v2.jsonl
dev_file_name: jianti_14-18_train+test_charlevel/detect_dev_v2.jsonl
load_test: 0
# evaluation_metric: 'posi_f1'
dropout: 0.1
max_seq_len: 256
train_batch_size: 256
eval_batch_size: 256
learning_rate: 1e-5
num_train_epochs: 10
no_cuda: False
# specify the GPU number
cuda_device: '4,5,6,7'
seed: 99
gradient_accumulation_steps: 2
over_write_cache: 1
resume_latest_cpt: 0
bad_case: 1
# save_cpt_flag value: {0: only save best model; 1: save best model & last epoch model; 2: save best model & each epoch model}
save_cpt_flag: 2
percent: 1.0